{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"zemberek.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"15TmY0gM8IxM7no4Ovuyz6I61DM5stKyt","authorship_tag":"ABX9TyNx6P4iLpUcnuHSP+/Oep7+"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SJKkL6C_rupk"},"source":["# start Java Virtual Machine for zemberek"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lMaR62Wzrt_z","executionInfo":{"status":"ok","timestamp":1612349354699,"user_tz":-180,"elapsed":2965,"user":{"displayName":"Eda ERSU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitxKeedZEG1l4vSaVrsdftaX6rhreVaUauh-YbHvQ=s64","userId":"13924854064862645209"}},"outputId":"18b38619-ad6b-418e-b026-ac3b8f275061"},"source":["pip install jpype1"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: jpype1 in /usr/local/lib/python3.6/dist-packages (1.2.1)\n","Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jpype1) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KKncRG4xUISs","executionInfo":{"status":"ok","timestamp":1612349355977,"user_tz":-180,"elapsed":389,"user":{"displayName":"Eda ERSU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitxKeedZEG1l4vSaVrsdftaX6rhreVaUauh-YbHvQ=s64","userId":"13924854064862645209"}}},"source":["import jpype"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"eMGTHgnSruEV","executionInfo":{"status":"ok","timestamp":1612349357098,"user_tz":-180,"elapsed":392,"user":{"displayName":"Eda ERSU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitxKeedZEG1l4vSaVrsdftaX6rhreVaUauh-YbHvQ=s64","userId":"13924854064862645209"}}},"source":["import jpype\r\n","from jpype import getDefaultJVMPath,JClass\r\n","jar = r\"drive/MyDrive/nlp/zemberek/zemberek-full.jar\"\r\n","if not jpype.isJVMStarted():\r\n","    jpype.startJVM(getDefaultJVMPath(), classpath=[jar])"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-OK2pdnSSK0","executionInfo":{"status":"ok","timestamp":1612349363266,"user_tz":-180,"elapsed":5537,"user":{"displayName":"Eda ERSU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitxKeedZEG1l4vSaVrsdftaX6rhreVaUauh-YbHvQ=s64","userId":"13924854064862645209"}}},"source":["TurkishMorphology = JClass('zemberek.morphology.TurkishMorphology')\r\n","TurkishSpellChecker = JClass('zemberek.normalization.TurkishSpellChecker')\r\n","TurkishSentenceNormalizer = JClass('zemberek.normalization.TurkishSentenceNormalizer')\r\n","Paths = JClass('java.nio.file.Paths')\r\n","lookupRoot = Paths.get(\"drive/MyDrive/nlp/normalization\")\r\n","lmPath = Paths.get(\"drive/MyDrive/nlp/data/lm/lm.2gram.slm\")\r\n","morphology = TurkishMorphology.createWithDefaults()\r\n","morph = TurkishMorphology.createWithDefaults()\r\n","spell = TurkishSpellChecker(morph)\r\n","#normalizer = TurkishSentenceNormalizer(morphology, lookupRoot, lmPath)"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ub3BDPqap9qN"},"source":["# stopwords, normalization, lemmatization, stemming "]},{"cell_type":"code","metadata":{"id":"8DvPU-PmYMvx"},"source":["data['preProcessing'] = data['preProcessing'].str.replace('i̇','i')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hmm9Ao5Vp8bi","executionInfo":{"status":"ok","timestamp":1612349490182,"user_tz":-180,"elapsed":424,"user":{"displayName":"Eda ERSU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitxKeedZEG1l4vSaVrsdftaX6rhreVaUauh-YbHvQ=s64","userId":"13924854064862645209"}}},"source":["import nltk\r\n","from nltk.corpus import stopwords\r\n","from string import punctuation, digits\r\n","import itertools\r\n","from pathlib import Path\r\n","import re\r\n","import os\r\n","from typing import List\r\n","from jpype import JClass, JString, getDefaultJVMPath, shutdownJVM, startJVM, java\r\n","import pandas as pd\r\n","from collections import Counter\r\n","from pathlib import Path\r\n","import numpy as np"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"nRZjjA2-qh7O"},"source":["import nltk\r\n","nltk.download(\"all\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hht9I_d6p8gQ","executionInfo":{"status":"ok","timestamp":1612349496187,"user_tz":-180,"elapsed":548,"user":{"displayName":"Eda ERSU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitxKeedZEG1l4vSaVrsdftaX6rhreVaUauh-YbHvQ=s64","userId":"13924854064862645209"}}},"source":["WPT = nltk.WordPunctTokenizer()\r\n","stop_word_list = nltk.corpus.stopwords.words('turkish')\r\n","stop_word_list.remove('ama')"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhMwXflbrSEC","executionInfo":{"status":"ok","timestamp":1612349502929,"user_tz":-180,"elapsed":3127,"user":{"displayName":"Eda ERSU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitxKeedZEG1l4vSaVrsdftaX6rhreVaUauh-YbHvQ=s64","userId":"13924854064862645209"}}},"source":["WORDS = dict()\r\n","spell_folder = Path(r\"drive/MyDrive/nlp/zemberek\")\r\n","\r\n","def words(text): return re.findall(r'\\w+', text.lower())\r\n","with open(os.path.expanduser(Path(spell_folder/ \"big2.txt\")), \"r\", encoding = 'utf-8') as f:\r\n","    for line in f:\r\n","        splitted = line.split()\r\n","        WORDS[splitted[0]] = int(splitted[1])"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"x6ymDc21q5pi","executionInfo":{"status":"ok","timestamp":1612349504822,"user_tz":-180,"elapsed":1068,"user":{"displayName":"Eda ERSU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitxKeedZEG1l4vSaVrsdftaX6rhreVaUauh-YbHvQ=s64","userId":"13924854064862645209"}}},"source":["def lower(text):\r\n","    text=re.sub(\"İ\",\"i\",text)\r\n","    text = text.lower()\r\n","    return text\r\n","\r\n","def resubComma(texts):\r\n","    texts= re.sub(\",\",\" \",texts)\r\n","    return texts\r\n","\r\n","def vanish_punc(text):\r\n","    regex = re.compile('[%s]' % re.escape(punctuation))\r\n","    text = regex.sub(' ', text)\r\n","    return text\r\n","\r\n","def vanish_digits(text):\r\n","    text=text.strip()\r\n","    vanish_digits = str.maketrans('', '', digits)\r\n","    text=text.translate(vanish_digits)\r\n","    return text\r\n","\r\n","def dup_vanish(s1):\r\n","     return (''.join(i for i, _ in itertools.groupby(s1)))\r\n","\r\n","def reverse(s): \r\n","    if len(s) == 0: \r\n","        return s \r\n","    else: \r\n","        return reverse(s[1:]) + s[0] \r\n","\r\n","def checkOpennes(word):\r\n","    vowels=['a','e','i','ı','o','ö','u','ü']\r\n","    open_vowels=['e','i','ü','ö']\r\n","    close_vowels=['a','ı','o','u']\r\n","    for i in range(len(word)):\r\n","        if reverse(word)[i] in vowels:\r\n","            if reverse(word)[i] in open_vowels:\r\n","                return True\r\n","            else:\r\n","                return False\r\n","        else:\r\n","            continue\r\n","def PresentCheck(word):\r\n","        ei=['e','i']\r\n","        aı=['a','ı']\r\n","        üö=['ü','ö']\r\n","        uo=['u','o']\r\n","        for i in range(len(word)):\r\n","            if reverse(word)[i] in ei:\r\n","                return 'ei'\r\n","            elif reverse(word)[i] in üö:\r\n","                return 'üö'\r\n","            elif reverse(word)[i] in aı:\r\n","                return 'aı'\r\n","            elif reverse(word)[i] in uo:\r\n","                return 'uo'\r\n","            else:\r\n","                continue\r\n","    \r\n","def StartCheck(word):\r\n","        ei=['e','i']\r\n","        aı=['a','ı']\r\n","        üö=['ü','ö']\r\n","        uo=['u','o']\r\n","        for i in range(len(word)):\r\n","            if (word)[i] in ei:\r\n","                return 'ei'\r\n","            elif (word)[i] in üö:\r\n","                return 'üö'\r\n","            elif (word)[i] in aı:\r\n","                return 'aı'\r\n","            elif (word)[i] in uo:\r\n","                return 'uo'\r\n","            else:\r\n","                continue\r\n","\r\n","def correction(word):\r\n","      return max(candi(word), key=P)"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"F6yPRKs4p8il","executionInfo":{"status":"ok","timestamp":1612349505401,"user_tz":-180,"elapsed":413,"user":{"displayName":"Eda ERSU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitxKeedZEG1l4vSaVrsdftaX6rhreVaUauh-YbHvQ=s64","userId":"13924854064862645209"}}},"source":["def replaceall(s, n,a):\r\n","    occurence = s.count(n)\r\n","    alt = []\r\n","    temp = s\r\n","    for i in range(occurence):\r\n","        temp2 = temp\r\n","        for j in range(i,occurence):\r\n","            temp2 = temp2.replace(n,a,1)\r\n","            alt.append(temp2)\r\n","        temp = temp.replace(n,\"!\",1)\r\n","    for i in range(len(alt)):\r\n","        alt[i] = alt[i].replace(\"!\",n)\r\n","\r\n","    return alt\r\n","\r\n","def P(word, N=sum(WORDS.values())):\r\n","    \"Probability of `word`.\"\r\n","    if  word in WORDS.keys():\r\n","        number = WORDS[word]\r\n","    else:\r\n","        number = 1\r\n","    if number == 0:\r\n","        number = 1\r\n","    return number / N\r\n","\r\n","def correction(word):\r\n","      return max(candi(word), key=P)\r\n","\r\n","def candi(word):\r\n","    \"Generate possible spelling corrections for word.\"\r\n","    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\r\n","\r\n","def known(words):\r\n","    \"The subset of `words` that appear in the dictionary of WORDS.\"\r\n","    return set(w for w in words if w in WORDS)\r\n","\r\n","def edits1(word):\r\n","    \"All edits that are one edit away from `word`.\"\r\n","    letters    = 'abcçdefgğhıijklmnoöprsştuüvyzw'\r\n","    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\r\n","    deletes    = [L + R[1:]               for L, R in splits if R]\r\n","    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\r\n","    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\r\n","    inserts    = [L + c + R               for L, R in splits for c in letters]\r\n","\r\n","    sp     = replaceall(word,'ı','i')\r\n","    sp2     = replaceall(word,'u','ü')\r\n","    sp3    = replaceall(word,'o','ö')\r\n","    sp4     = replaceall(word,'g','ğ')\r\n","    sp5     = replaceall(word,'c','ç')\r\n","    sp6     = replaceall(word,'s','ş')\r\n","    sp7     = replaceall(word,'i','ı')\r\n","    sp8     = replaceall(word,'ö','o')\r\n","    sp9     = replaceall(word,'ş','s')\r\n","    sp10     = replaceall(word,'ğ','g')\r\n","    sp11     = replaceall(word,'ç','c')\r\n","    sp12     = replaceall(word,'ü','u')\r\n","    specials=[]\r\n","    specials.extend(sp)\r\n","    specials.extend(sp2)\r\n","    specials.extend(sp3)\r\n","    specials.extend(sp4)\r\n","    specials.extend(sp5)\r\n","    specials.extend(sp6)\r\n","    specials.extend(sp7)\r\n","    specials.extend(sp8)\r\n","    specials.extend(sp9)\r\n","    specials.extend(sp10)\r\n","    specials.extend(sp11)\r\n","    specials.extend(sp12)\r\n","    return set(deletes+transposes+replaces+inserts+specials)\r\n","\r\n","def edits2(word):\r\n","    \"All edits that are two edits away from `word`.\"\r\n","    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\r\n","def print_diff(word, s):\r\n","    if not word == s:\r\n","        print(word + \" --> \" + s)\r\n","counter = 0"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"_mzpafsJp8pM","executionInfo":{"status":"ok","timestamp":1612349506306,"user_tz":-180,"elapsed":418,"user":{"displayName":"Eda ERSU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitxKeedZEG1l4vSaVrsdftaX6rhreVaUauh-YbHvQ=s64","userId":"13924854064862645209"}}},"source":["def lemmatizer(word,texts):\r\n","        wordList=[]\r\n","        wordList = re.sub(\"[^\\w]\", \" \",  texts).split()\r\n","        if '�' in word:\r\n","            return 'question'\r\n","        pos=wordList.index(word)\r\n","        sakin=''\r\n","        word=correction(word)\r\n","        ## word=yaşında\r\n","        if len(wordList)-pos>3 and pos>2:\r\n","            for i, kelime in enumerate(wordList[pos-3:pos+4]):\r\n","                sakin=sakin+correction(kelime)+' '\r\n","        elif pos<=2 and len(wordList)-pos>5:\r\n","            for i, kelime in enumerate(wordList[pos:pos+5]):\r\n","                sakin=sakin+correction(kelime)+' '\r\n","        elif pos<=2 and len(wordList)-pos<=5:\r\n","            for i, kelime in enumerate(wordList[pos:len(wordList)]):\r\n","                sakin=sakin+correction(kelime)+' '\r\n","        elif len(wordList)-pos<1 and pos>3:\r\n","            for i, kelime in enumerate(wordList[pos-3:len(wordList)]):\r\n","                sakin=sakin+correction(kelime)+' '\r\n","        elif len(wordList)<3:\r\n","            for i, kelime in enumerate(wordList):\r\n","                sakin=sakin+correction(kelime)+' '\r\n","        else:\r\n","             for i, kelime in enumerate(wordList):\r\n","                sakin=sakin+correction(kelime)+' '\r\n","        results = morphology.analyze(word)\r\n","        lemma=[]\r\n","        form=[]\r\n","        l=[]\r\n","        m=[]\r\n","        for i, result in enumerate(results):\r\n","            form.append(str(result.formatLong()))\r\n","            lemma.append(result.getLemmas()[0])\r\n","        if len(lemma)>1:\r\n","                analysis = morphology.analyzeSentence(sakin)\r\n","                results = morphology.disambiguate(sakin, analysis).bestAnalysis()\r\n","                \r\n","                for i, result in enumerate(results):\r\n","                        l.append(result.getLemmas()[0])\r\n","                        m.append(result.formatLong())\r\n","                for i in range(len(m)):\r\n","                    for j in range(len(form)):\r\n","                        if m[i]==form[j]:\r\n","                            lema=lemma[j]\r\n","                            if lema=='değil':\r\n","                                return 'değil'\r\n","                            if 'Neg' in form[j] or 'WithoutHavingDoneSo' in form[j] or 'Unable' in form[j]:\r\n","                                if checkOpennes(word):\r\n","                                    return lema+'me'\r\n","                                else:\r\n","                                    return lema+'ma'\r\n","                            if 'Without' in form[j]:\r\n","                                if PresentCheck(word)=='ei':\r\n","                                    return lema+'siz'\r\n","                                elif PresentCheck(word)=='aı':\r\n","                                    return lema+'sız'\r\n","                                elif PresentCheck(word)=='uo':\r\n","                                    return lema+'suz'\r\n","                                else:\r\n","                                    return lema+'süz'\r\n","                            if 'With' in form[j]:\r\n","                                if PresentCheck(word)=='ei':\r\n","                                    return lema+'li'\r\n","                                elif PresentCheck(word)=='aı':\r\n","                                    return lema+'lı'\r\n","                                elif PresentCheck(word)=='uo':\r\n","                                    return lema+'lu'\r\n","                                else:\r\n","                                    return lema+'lü'\r\n","                            else:\r\n","                                return lema\r\n","                    else:\r\n","                        continue\r\n","        elif len(lemma)==1:\r\n","            if lemma[0]=='değil':\r\n","                return lemma[0]\r\n","            if 'Neg' in form[0] or 'WithoutHavingDoneSo' in form[0] or 'Unable' in form[0]:\r\n","                 if checkOpennes(word):\r\n","                    return lemma[0]+'me'\r\n","                 else:\r\n","                    return lemma[0]+'ma'\r\n","            elif 'Without' in form[0]:\r\n","                if PresentCheck(word)=='ei':\r\n","                    return lemma[0]+'siz'\r\n","                elif PresentCheck(word)=='aı':\r\n","                    return lemma[0]+'sız'\r\n","                elif PresentCheck(word)=='uo':\r\n","                    return lemma[0]+'suz'\r\n","                else:\r\n","                    return lemma[0]+'süz'\r\n","            elif 'With' in form[0]:\r\n","                if PresentCheck(word)=='ei':\r\n","                    return lemma[0]+'li'\r\n","                elif PresentCheck(word)=='aı':\r\n","                    return lemma[0]+'lı'\r\n","                elif PresentCheck(word)=='uo':\r\n","                    return lemma[0]+'lu'\r\n","                else:\r\n","                    return lemma[0]+'lü'\r\n","            else:\r\n","                return lemma[0]\r\n","        else:\r\n","            return word"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"aRAp4lqGrbKD","executionInfo":{"status":"ok","timestamp":1612349507360,"user_tz":-180,"elapsed":576,"user":{"displayName":"Eda ERSU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitxKeedZEG1l4vSaVrsdftaX6rhreVaUauh-YbHvQ=s64","userId":"13924854064862645209"}}},"source":["def clean_messages(texts):\r\n","    texts=lower(texts)\r\n","    texts=resubComma(texts)\r\n","    tokens=WPT.tokenize(texts)\r\n","    text = [token for token in tokens if token not in stop_word_list]\r\n","\r\n","    for i, word in enumerate(text):\r\n","        text[i]=dup_vanish(vanish_digits(vanish_punc((word))))\r\n","\r\n","    text = list(filter(None,text))\r\n","    metin = ' '.join(text)\r\n","\r\n","    for i, word in enumerate(text):\r\n","         if(word==\" \"):\r\n","           continue\r\n","         text[i] = lemmatizer(word,metin)\r\n","\r\n","    return ' '.join(''.join(elems) for elems in text)\r\n"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"zZOcu1djoOT4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vyQDXQDQoOiF"},"source":["# Uygulama"]},{"cell_type":"code","metadata":{"id":"H1Iwl-tnX936","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1612351307916,"user_tz":-180,"elapsed":376,"user":{"displayName":"Eda ERSU","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GitxKeedZEG1l4vSaVrsdftaX6rhreVaUauh-YbHvQ=s64","userId":"13924854064862645209"}},"outputId":"a312c9f0-5476-4c28-a1df-08def428e3de"},"source":["## deneme\r\n","clean_messages(\"dişarida hava cok guzel\")"],"execution_count":136,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'dışarı hava çok güzel'"]},"metadata":{"tags":[]},"execution_count":136}]},{"cell_type":"code","metadata":{"id":"vUG2xZQM8Q-B"},"source":["%%time\r\n","data[\"clean_messages\"] = data[\"preProcessing\"].map(clean_messages)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_nEQdvmghBjt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CcYOq5rEoGdw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cSVUpdQeoGgY"},"source":[""],"execution_count":null,"outputs":[]}]}